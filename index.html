<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Predicting the Results of Primaries through Twitter">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Predicting the Results of Primaries through Twitter</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/sbalanovich/APM115Proj2">View on GitHub</a>

          <h1 id="project_title">Predicting the Results of Primaries through Twitter</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/sbalanovich/APM115Proj2/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/sbalanovich/APM115Proj2/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>
<a id="goal-and-background" class="anchor" href="#goal-and-background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goal and Background</h2>

<p>The season of campaigns, debates, and primaries and caucuses leading up to the 2016 presidential election has undoubtedly been one of most eventful, widely discussed, and exciting in living memory: the members of each party consider candidates largely censured by the entrenched party establishments whose opinions and policy proposals range from what might be considered ultra-conservative to what might be considered ultra-liberal.</p>

<p>No little interest in the on-going events is generated by those who would forecast the outcome of the primaries and, ultimately, the election. After the widely acknowledged success of Nate Silver in his predictions of the 2012 election, the possibility of another sweeping “victory” on that field through the use of data-driven methods has become a most tantalizing prospect.
Yet other sources of the enthusiasm surrounding the primary are the incredibly democratic forums that are the social media. Across Facebook, Twitter, and Tumblr, countless individuals weigh in on the election, providing opinions, statements of support, and even their own predictions as to its outcome.</p>

<p>It seemed to us that this second series of sources remained largely untapped by the first: whereas predictions from many polls and aggregators are circulated through social media, we find that not many attempt to incorporate data from, for example, Facebook statuses and Twitter Tweets despite the immense population participating in the latter that is likely to be non-intersecting with the population participating in the former.</p>

<p>As such, we chose to investigate this latter outlet of information. In this project, we tried primarily to see whether inclusion of data gathered from Twitter could augment the predictive power of models aiming to predict the county-by-county results of primary elections and secondarily to build such a model based on campaign finances (contributions and expenditures). Specifically, we sought to build a model from the results of several April primaries and caucuses and then to predict the results of another.</p>

<h2>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

<p>One of the great difficulties in this project was procuring suitable, representative, and complete pertinent data. In particular, we needed to compile a large body of Tweets as well as to collect information concerning the campaign finances of the five candidates.
The second of these tasks was straightforward, as such is contained in well documented public records: the Federal Election Commission stores such information in easy-to-parse .csv files.</p>

<p>Collecting Tweets, however, proved to be extremely difficult and limiting with respect to the potential scope of our project. We had two avenues to Tweet access: the Search Representational State Transfer Application Programming Interface and the Streaming APIs. The former provides programmatic access to read public Twitter data from the past seven days while the latter provides access to read Twitter’s global stream of Tweet data. Thus was the first difficulty: neither mode of access provided the ability to retrieve historical data, which meant that the data we could use to train our model was constrained to that associated with primaries and caucuses taking place in April—which meant that we would need to train on the data provided by the New York primaries and test on that provided by the Connecticut primaries.</p>

<p>There were further limitations inherent in the APIs however. It was never the case that we received all of the Tweets across Twitter, or even all of the publically available Tweets – we merely saw what Twitter deigned to provide us with, which was likely to be less than a single percent of the global traffic. We were also ultimately unable to use the Streaming API – the manner in which it narrowed down Tweets according to search parameters (i.e., barely at all) caused us to receive many gigabytes of data that we could not process. The streaming nature of the API also prevented our use of it, as we had no method of setting up a constantly running endpoint to collect such data.</p>

<p>What remained to us was the collection and sifting through of Twitter Search API data. We collected Tweets containing any of several hashtags, about four per candidate: <code>feelthebern</code>, <code>sanders</code>, <code>berniesanders</code>, <code>bernie2016</code>, <code>Clinton</code>, <code>hillaryclinton</code>, <code>imwithher</code>, <code>hillary2016</code>, <code>tedcruz</code>, <code>choosecruz</code>, <code>cruzcrew</code>, <code>unitewithcruz</code>, <code>cruz2016</code>, <code>trump2016</code>, <code>donaldtrump</code>, <code>#trump</code>, <code>donaldtrump2016</code>, <code>kasichcan</code>, <code>johnkasich</code>, <code>kasich2016</code>, <code>kasich4us</code>. These were additionally associated with belonging to one specific county (one of the sixty-two in New York for training, or one of the eight in Connecticut for testing). Overall, we collected around 60000 Tweets from the week leading up to the New York primaries and around 24000 Tweets from the week leading up to the Connecticut primaries. Our Tweet collection from these states may be summarized by the maps below, where a single blue dot indicates a geocoded Tweet.</p>

<p><img src="https://raw.githubusercontent.com/sbalanovich/APM115Proj2/gh-pages/images/NYTweetsMap.png" alt="New York Map"></p>

<p><img src="https://raw.githubusercontent.com/sbalanovich/APM115Proj2/gh-pages/images/CTTweetsMap.png" alt="Connecticut Map"></p>

<p>We note that the Tweets are exceedingly sparse except in major cities like New York City or New Haven.</p>

<h2>
<a id="approachmodel" class="anchor" href="#approachmodel" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach/Model</h2>

<p>Armed with these data, we started by exploring and considering the best way to model them. We began by identifying the most important features of our dataset and examining their interplay. Given the limitations on the tweet data and given its importance to the modeling goals of the project, we decided to tailor the model to these limitations. Thus, instead of creating a state-based model wherein we might use the primary outcomes of many states to predict those of several others, we limited our modeling effort to the creation of a model wherein we use the primary outcomes the 62 counties of NY to predict results for the 8 counties of CT. As poll results were not computed by county, these were no longer considered as a part of the model, and the main focus was directed at the finance and Tweet data on a per-county level.</p>

<p>For campaign finances, there were three fundamental attributes that were assumed to be at all plausible in predicting election outcomes: the amount a candidate raised (signaling the amount of active support for the candidate), the amount a candidate spent (indicating the efforts exerted to sway a voter base), and these amounts in the final week(s) leading up to the election (the growth rate of which would indicate the existence of a surge in the support for or efforts of a candidate). As such, for each county we considered the total financial sum contributed or spent over the entire campaign, the average sum from each contributor or campaign event, and these same two features limited to the final two weeks of the campaign.</p>

<p>On the Twitter front, we isolated Tweets that were not Retweets and specifically geolocated to the counties in question from the rest. It was hypothesized that the posters of these Tweets, seemingly more politically active than those who merely pressed a Retweet button, would have a higher degree of participation in the discussion and persuasion concerning the election. For each of the two sets (not-Retweets and all Tweets), we identified features such as the average number of followers of a user who posted a Tweet pertaining to a certain candidate, the average number of Retweets of Tweets pertaining to a certain candidate, and even the proportion of Tweets pertaining to a certain candidate with a specific sentiment as evaluated by the Sentiment140 API. This API was intended to provide the better metric of “positive Tweets” rather than “total Tweets,” as it seems that the former would be more relevant to the success of a candidate. To further compress this feature set to be more manageable, the ratio of each feature for a given candidate to the total was taken. For instance, in the Democratic predictions, the features “Clinton Total RT” and “Sanders Total RT” (where RT stands for Retweets) were compressed into the single feature “Clinton:Sanders (ratio) Total RT” without loss of information.</p>

<p>All told, we had assembled some 38 features for the Democratic race and 64 features for the Republican race (the latter is greater because there was an additional Republican candidate at the time). However, reconsidering our stated goal, we realized that we desired a model that would be able to tell us the relative importance of each of our features, both because we desired to discover whether the Tweet-related features were important to better modeling votes and because we had so many features relative to the number of data points.</p>

<p>Given the limitations of the data and the goals of the modeling, we decided to employ the ensemble machine learning method known as Random Forest Regression. Random Forest (RF) Regression involves the fitting of Decision Trees to the training data and then combining the results over these trees based on effective feature and training set sampling. Fundamentally, a Decision Tree is a classification representation of a dataset, which uses the features developed for the model to classify the data points based on which categories they fall into. Decision Trees are therefore most commonly used for classification purposes, but they can be modified to discretize continuous spaces and therefore serve as a form of regression, which is the way the approach was used in this project. Through repeated partitioning of the dataset based on the provided features, the Decision Tree Regressor constructs a model that can, with reasonable accuracy, classify each of the training points into their appropriate target buckets, thereby creating a very accurate model for the specific data that was used to construct it.</p>

<p>Decision Trees use probabilities and proportions to determine their classifications, so they are able to remain invariant under the scaling of features. Additionally, the classification is such that if a feature does not contribute to the classification of many data points (and is therefore fairly irrelevant), it does not affect the overall Decision Tree as it simply gets ignored. Finally, the tree is by definition an inspectable model, allowing us to examine its weighting and utilization of the features in the classification of the final product. All of these traits were very desirable, as the goal of the project was to test a large number of features, to avoid obfuscation of the model by useless ones, and to ultimately inspect the model and determine whether any of the features we tried were at all effective.</p>

<p>However, it is also clear that by construction, these Decision Trees overfit the data severely. Because they have such strict classification rules and discard features that are seemingly irrelevant for the particular training set they are run on, it is very likely that a single tree will be a terrible predictor for any other data. As a result, it cross-validation, these trees perform rather poorly and need to be augmented in some way to make them more effective. For this reason, the Random Forest approach is adopted. The Random Forest performs something known as tree bagging - it takes a random sample of the training data (with replacement) and fits a decision tree to it. Beyond tree bagging, the RF approach also takes a random sample of the provided features and only trains trees on these. This way, if many trees are trained and averaged over a number of these feature subsets, the most prominent features across all the trees will have the greatest effect on the final result. Through this feature bagging approach, the RF Regressor manages to isolate the most impactful features for a subset of the data. This is done over several such random subsets and when the model is complete, at prediction time, the mean of all the decision trees’ predictions is provided as output.</p>

<p>This approach allows us to maintain the beneficial qualities of decision trees - filtering out unnecessary features and observing those that are most important - while avoiding the general issue of overfitting the data.</p>

<h2>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>

<p>On the whole, our results can only be described as inconclusive; we do not have enough data points to be sure of the answer to either of our questions: We do not know whether Twitter augments our base model—it seems to in fact worsen it, but this is likely due to the large number of additional features the Twitter data adds; and our model generates extremely weak predictions.
Specifically, the following are graphics that depict the predictions on our testing data by the model that incorporates all financial and Twitter-related features:</p>

<p><img src="https://raw.githubusercontent.com/sbalanovich/APM115Proj2/gh-pages/images/ConnecticutDemPredictions.png" alt="Connecticut Democratic predictions"></p>
<p><img src="https://raw.githubusercontent.com/sbalanovich/APM115Proj2/gh-pages/images/ConnecticutRepPredictions.png" alt="Connecticut Republican predictions"></p>

<p>These error bars were generated by predicting the vote proportion by use of each of the trees that constitutes the random forest and then taking the 2.5% and 97.5% quantiles. It is evident that our predictions are far off and extremely uncertain. (NB: These predictions are different from the ones listed in our presentation because there was a bug in our code that caused our predictions to be far closer than they should have been to the actual values.)
We note that one county overwhelmingly voted for Cruz and another for Kasich, resulting in the red points far outside of the intervals.
The following two sets of tables depict a comparison of the different models that we used. The first set contains tables of the Mean Absolute Error of the predictions over each dataset for each model (NB: the term R^2 used in the presentation referred to the difference between unity and the ratio of the residual sum of squares and the total sum of squares, which can result in a negative number if the former is much greater than the latter). Each set of tables begins with the models' predictions on the Democratic race:</p>

<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Training</th>
<th align="center">Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Contributions Over Time</td>
<td align="center">0.022996</td>
<td align="center">0.068252</td>
</tr>
<tr>
<td align="left">Contributions</td>
<td align="center">0.021971</td>
<td align="center">0.034600</td>
</tr>
<tr>
<td align="left">Expenditures Over Time</td>
<td align="center">0.047842</td>
<td align="center">0.031352</td>
</tr>
<tr>
<td align="left">Expenditures</td>
<td align="center">0.047734</td>
<td align="center">0.038127</td>
</tr>
<tr>
<td align="left">C/E Over Time</td>
<td align="center">0.022386</td>
<td align="center">0.068348</td>
</tr>
<tr>
<td align="left">C/E</td>
<td align="center">0.025281</td>
<td align="center">0.040461</td>
</tr>
<tr>
<td align="left">Twitter Only</td>
<td align="center">0.027176</td>
<td align="center">0.042261</td>
</tr>
<tr>
<td align="left">C/E and Twitter</td>
<td align="center">0.023269</td>
<td align="center">0.066317</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Training</th>
<th align="center">Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Contributions Over Time</td>
<td align="center">0.021354</td>
<td align="center">0.261101</td>
</tr>
<tr>
<td align="left">Contributions</td>
<td align="center">0.021403</td>
<td align="center">0.259831</td>
</tr>
<tr>
<td align="left">Expenditures Over Time</td>
<td align="center">0.046962</td>
<td align="center">0.251923</td>
</tr>
<tr>
<td align="left">Expenditures</td>
<td align="center">0.048078</td>
<td align="center">0.252177</td>
</tr>
<tr>
<td align="left">C/E Over Time</td>
<td align="center">0.021646</td>
<td align="center">0.258757</td>
</tr>
<tr>
<td align="left">C/E</td>
<td align="center">0.021664</td>
<td align="center">0.258880</td>
</tr>
<tr>
<td align="left">Twitter Only</td>
<td align="center">0.035601</td>
<td align="center">0.230869</td>
</tr>
<tr>
<td align="left">C/E and Twitter</td>
<td align="center">0.020891</td>
<td align="center">0.240943</td>
</tr>
</tbody>
</table>

<p>And this set of tables indicates the success of our model as it pertains to predicting the correct winner in each county: it lists the number of counties for each model whose winner was successfully predicted in each race.</p>

<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Training (out of 62)</th>
<th align="center">Testing (out of 8)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Contributions Over Time</td>
<td align="center">58</td>
<td align="center">4</td>
</tr>
<tr>
<td align="left">Contributions</td>
<td align="center">58</td>
<td align="center">8</td>
</tr>
<tr>
<td align="left">Expenditures Over Time</td>
<td align="center">53</td>
<td align="center">8</td>
</tr>
<tr>
<td align="left">Expenditures</td>
<td align="center">53</td>
<td align="center">8</td>
</tr>
<tr>
<td align="left">C/E Over Time</td>
<td align="center">60</td>
<td align="center">4</td>
</tr>
<tr>
<td align="left">C/E</td>
<td align="center">57</td>
<td align="center">7</td>
</tr>
<tr>
<td align="left">Twitter Only</td>
<td align="center">58</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">C/E and Twitter</td>
<td align="center">59</td>
<td align="center">5</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Training (out of 62)</th>
<th align="center">Testing (out of 8)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Contributions Over Time</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">Contributions</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">Expenditures Over Time</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">Expenditures</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">C/E Over Time</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">C/E</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">Twitter Only</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">C/E and Twitter</td>
<td align="center">62</td>
<td align="center">6</td>
</tr>
</tbody>
</table>

<h2>
<a id="future-directions" class="anchor" href="#future-directions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Future Directions</h2>

<p>This project establishes the baseline for using Twitter data to predict election outcomes or to augment other models. It shows that, while Twitter is not necessarily the best tool for predictions of this kind, even with the limited data and small training and test sets, the results were still promising. Future work in this area could take one of two directions - either the Twitter modeling in general could be improved through better and more comprehensive data collection, or the baseline model which the Twitter data was meant to augment could be more comprehensive.</p>

<p>For the first approach, better access to more data would be critical. This could likely be achieved through a paid service allowing for 100%, rather than a mere 1%, of the Twitter data to be collected about a given state with no limitations on time. This would allow for more fine-grained control over the time spans over which Tweets are collected and would permit better filtering and feature extraction from these same Tweets. A more powerful and accurate sentiment analysis tool could be used to better classify Tweet sentiments, which would also likely generate more effective features. Such paid services would also allow the model to be extended beyond the very limited scope of NY and CT and would allow for the evaluation of many more states, increasing the size of the training set and yielding more comprehensive results.</p>

<p>For the second direction, the polling and demographic data that were not used in this project as a result of the county limitations and the feature oversaturation could be introduced into a state model. Rather than basing a training set on the 62 counties of NY, a training set of 20 states could be built based on the polls, finances, and demographics of those states. These could then be used to predict the outcome of the primary within a single state, and the limited Tweet data could be used to try and augment these predictions. Ultimately, either by building a better Twitter-based model or by building a better initial model to augment with Twitter data, this project’s work could be extended to determine more rigorously the efficacy of using Twitter data for these kinds of predictions.</p>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

<p><ul>
  <li>All of the code written for this project was done in Python using <a href="http://ipython.readthedocs.io/en/stable/index.html">IPython Notebooks</a>.</li>
  <li>Figures were created using <a href="http://matplotlib.org/">matplotlib</a> and <a href="http://www.numpy.org/">NumPy</a>.</li>
  <li><code>scikit-learn</code>'s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">RandomForestRegressor</a> was used to build our model.</li>
  <li>Twitter data was compiled using the <a href="https://dev.twitter.com/rest/public/search">Twitter Search API</a>.</li>
  <li>Campaign financial data was compiled from information provided by the <a href="http://www.fec.gov/">Federal Election Commission</a>.</li>
  <li>Presidential primary outcomes were retrieved from <a href="http://www.politico.com/2016-election/results/map/president">POLITICO</a>.</li>
  <li><a href="http://www.sentiment140.com/">Sentiment140</a> was used to tag individual Tweets' sentiments as positive, negative, or neutral.</li>
  <li>Shi, Lei, et al. "Predicting US primary elections with Twitter." URL: <a href="http://snap.stanford.edu/social2012/papers/shi.pdf">http://snap.stanford.edu/social2012/papers/shi.pdf</a> (2012).</li>
</ul></p>

<h2>
<a id="appendix" class="anchor" href="#appendix" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Appendix</h2>

<p>A great deal of code was written in Python for this project. The GitHub repository that contains this code is linked at the top of this writeup page. The following is a brief listing of the individual notebooks that comprise the body:</p>

<p><ul>
  <li><a href="https://github.com/sbalanovich/APM115Proj2/blob/master/TwitterSearch.ipynb">TwitterSearch.ipynb</a> contains the code used to retrieve Tweets from the Twitter Search API.</li>
  <li><a href="https://github.com/sbalanovich/APM115Proj2/blob/master/Feature%20Extraction.ipynb">Feature Extraction.ipynb</a> contains the code used to create DataFrames containing Tweet-derived features.</li>
  <li><a href="https://github.com/sbalanovich/APM115Proj2/blob/master/Random%20Forest%20Tests.ipynb">Random Forest Tests.ipynb</a> contains the code used to extract features from campaign financial data as well as code used to test and visualize our model.</li>
  <li>The <a href="https://github.com/sbalanovich/APM115Proj2/tree/master/pickles">pickles</a> folder contains several serialized data files created in the above Notebooks. Not all of the created pickles are found here, as they are larger than GitHub's 100 MB per-file limit.</li>
  <li>The <a href="https://github.com/sbalanovich/APM115Proj2/tree/master/CSVs">CSVs</a> folder contains a number of CSVs used and created throughout the above notebooks. They include the per-county Twitter geocode data as well as the financial data retrieved from the FEC.</li>
  <li>The remaining code is largely scratchwork.</li>
</ul></p>

<h2>
<a id="attribution-of-individual-effort" class="anchor" href="#attribution-of-individual-effort" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Attribution of Individual Effort</h2>

<p>Serguei conducted the campaign finances data gathering and feature extraction. Elmer conducted the Twitter data gathering and feature extraction. Each contributed to the modeling. Serguei created the majority of the presentation. Elmer created most of the visuals wrote a large portion of the writeup.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Predicting the Results of Primaries through Twitter maintained by <a href="https://github.com/elmertan">elmertan</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
