{
  "name": "Testing2",
  "tagline": "",
  "body": "## Goal and Background\r\nThe season of campaigns, debates, and primaries and caucuses leading up to the 2016 presidential election has undoubtedly been one of most eventful, widely discussed, and exciting in living memory: the members of each party consider candidates largely censured by the entrenched party establishments whose opinions and policy proposals range from what might be considered ultra-conservative to what might be considered ultra-liberal.\r\n\r\nNo little interest in the on-going events is generated by those who would forecast the outcome of the primaries and, ultimately, the election. After the widely acknowledged success of Nate Silver in his predictions of the 2012 election, the possibility of another sweeping “victory” on that field through the use of data-driven methods has become a most tantalizing prospect.\r\nYet other sources of the enthusiasm surrounding the primary are the incredibly democratic forums that are the social media. Across Facebook, Twitter, and Tumblr, countless individuals weigh in on the election, providing opinions, statements of support, and even their own predictions as to its outcome.\r\n\r\nIt seemed to us that this second series of sources remained largely untapped by the first: whereas predictions from many polls and aggregators are circulated through social media, we find that not many attempt to incorporate data from, for example, Facebook statuses and Twitter Tweets despite the immense population participating in the latter that is likely to be non-intersecting with the population participating in the former.\r\n\r\nAs such, we chose to investigate this latter outlet of information. In this project, we tried primarily to see whether inclusion of data gathered from Twitter could augment the predictive power of models aiming to predict the county-by-county results of primary elections and secondarily to build such a model based on campaign finances (contributions and expenditures). Specifically, we sought to build a model from the results of several April primaries and caucuses and then to predict the results of another.\r\n\r\n## Data\r\n\r\nOne of the great difficulties in this project was procuring suitable, representative, and complete pertinent data. In particular, we needed to compile a large body of Tweets as well as to collect information concerning the campaign finances of the five candidates.\r\nThe second of these tasks was straightforward, as such is contained in well documented public records: the Federal Election Commission stores such information in easy-to-parse .csv files.\r\n\r\nCollecting Tweets, however, proved to be extremely difficult and limiting with respect to the potential scope of our project. We had two avenues to Tweet access: the Search Representational State Transfer Application Programming Interface and the Streaming APIs. The former provides programmatic access to read public Twitter data from the past seven days while the latter provides access to read Twitter’s global stream of Tweet data. Thus was the first difficulty: neither mode of access provided the ability to retrieve historical data, which meant that the data we could use to train our model was constrained to that associated with primaries and caucuses taking place in April—which meant that we would need to train on the data provided by the New York primaries and test on that provided by the Connecticut primaries.\r\n\r\nThere were further limitations inherent in the APIs however. It was never the case that we received all of the Tweets across Twitter, or even all of the publically available Tweets – we merely saw what Twitter deigned to provide us with, which was likely to be less than a single percent of the global traffic. We were also ultimately unable to use the Streaming API – the manner in which it narrowed down Tweets according to search parameters (i.e., barely at all) caused us to receive many gigabytes of data that we could not process. The streaming nature of the API also prevented our use of it, as we had no method of setting up a constantly running endpoint to collect such data.\r\n\r\nWhat remained to us was the collection and sifting through of Twitter Search API data. We collected Tweets containing any of several hashtags, about four per candidate: `feelthebern`, `sanders`, `berniesanders`, `bernie2016`, `Clinton`, `hillaryclinton`, `imwithher`, `hillary2016`, `tedcruz`, `choosecruz`, `cruzcrew`, `unitewithcruz`, `cruz2016`, `trump2016`, `donaldtrump`, `#trump`, `donaldtrump2016`, `kasichcan`, `johnkasich`, `kasich2016`, `kasich4us`. These were additionally associated with belonging to one specific county (one of the sixty-two in New York for training, or one of the eight in Connecticut for testing). Overall, we collected around 60000 Tweets from the week leading up to the New York primaries and around 24000 Tweets from the week leading up to the Connecticut primaries. Our Tweet collection from these states may be summarized by the maps below, where a single blue dot indicates a geocoded Tweet.\r\n\r\n![Blah](https://github.com/)\r\n\r\nWe note that the Tweets are exceedingly sparse except in major cities like New York City or New Haven.\r\n\r\n## Approach/Model\r\n\r\nArmed with these data, we started by exploring and considering the best way to model them. We began by identifying the most important features of our dataset and examining their interplay.\r\nWith respect to campaign finances, we created features based on contributions and expenditures within a county over the weeks leading up to a primary.\r\n\r\nOn the Twitter front, we isolated Tweets that were not Retweets and specifically geolocated to the counties in question from the rest: the posters of these Tweets, we thought, have so indicated a higher degree of participation in the discussion and persuasion concerning the election than those who merely pressed a Retweet button. For each of the two sets (not-Retweets and all Tweets) we identified features such as the average number of followers of a user who posted a Tweet pertaining to a certain candidate, the average number of Retweets of Tweets pertaining to a certain candidate, and even the proportion of Tweets pertaining to a certain candidate with a specific sentiment as evaluated by the Sentiment140 API.\r\n\r\nAll told, we had assembled some 38 features for the Democratic race and 64 features for the Republican race (the latter is greater because there was an additional Republican candidate at the time). However, reconsidering our stated goal, we realized that we desired a model that would be able to tell us the relative importance of each of our features, both because we desired to discover whether the Tweet-related features were important to better modeling votes and because we had so many features relative to the number of data points.\r\n\r\nAs such, we decided upon the use of an ensemble learning method called random forests. At training time, we construct a “forest” of decision trees to fit the training data. At prediction time, the mean of the decision trees’ predictions is output. This method was chosen because it would allow us to examine the importance of our features (as a result of the use of decision trees) but also correct for the inherent nature of decision trees to overfit.\r\n\r\n## Results\r\n\r\nOn the whole, our results can only be described as inconclusive; we do not have enough data points to be sure of the answer to either of our questions: We do not know whether Twitter augments our base model—it seems to in fact worsen it, but this is likely due to the large number of additional features the Twitter data adds; and our model generates extremely weak predictions.\r\nSpecifically, the following is a graphic that depicts the predictions on our testing data (Democrat-only, simply because the Republican results are similar and we choose to omit them for the sake of parsimony) by the model that incorporates all financial and Twitter-related features:\r\n \r\nThese error bars were generated by predicting the vote proportion by use of each of the trees that constitutes the random forest and then taking the 2.5% and 97.5% quantiles. It is evident that our predictions are far off and extremely uncertain. (NB: These predictions are different from the ones listed in our presentation because there was a bug in our code that caused our predictions to be far closer than they should have been to the actual values.)\r\nThe following two tables depict a comparison of the different models that we used. The first is a table of Mean Absolute Error of the predictions over each dataset for each model (NB: the term R^2 used in the presentation referred to the difference between unity and the ratio of the residual sum of squares and the total sum of squares, which can result in a negative number if the former is much greater than the latter):\r\n\r\nModel |\tTraining | Testing\r\n:--- | :---: | :---:\r\nContributions Over Time | 0.022996 | 0.068252\r\nContributions | 0.021971 | 0.034600\r\nExpenditures Over Time | 0.047842 | 0.031352\r\nExpenditures | 0.047734 | 0.038127\r\nC/E Over Time | 0.022386 | 0.068348\r\nC/E | 0.025281 | 0.040461\r\nTwitter Only | 0.027176\t| 0.042261\r\nC/E and Twitter | 0.023269 | 0.066317\r\n\r\nAnd this table indicates the success of our model as it pertains to predicting the correct winner in each county: it lists the number of counties for each model whose winner was successfully predicted in the Democratic race.\r\n\r\nModel | Training (out of 62) | Testing (out of 8)\r\n:--- | :---: | :---:\r\nContributions Over Time | 58 | 4\r\nContributions | 58 | 8\r\nExpenditures Over Time | 53 | 8\r\nExpenditures | 53 | 8\r\nC/E Over Time | 60 | 4\r\nC/E | 57 | 7\r\nTwitter Only | 58 | 6\r\nC/E and Twitter | 59 | 5\r\n\r\n## Future Directions\r\n\r\nThis project leaves a great deal for future investigation, as the modeling and analyses conducted herein were very much first steps rather than a comprehensive approach: with more suitable infrastructure, we could better collect and filter Tweets to better gather data; with better timing and more time, we could have been modeling on larger training and testing sets.\r\n\r\n## What We Learned\r\n\r\nI don’t think we need this section.\r\n\r\n## References:\r\n\r\n?\r\n\r\n## Attribution of Individual Effort:\r\n\r\nSerguei conducted the campaign finances data gathering and feature extraction. Elmer conducted the Twitter data gathering and feature extraction. Each contributed to the modeling. Serguei created the majority of the presentation. Elmer created the visuals wrote the majority of the writeup.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}